---
title: "Generalized Linear Models with the **rstanarm** R Package"
author: "Ben Goodrich"
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: yes
editor_options: 
  chunk_output_type: console
params:
  class: FALSE
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

```{r setup, include=FALSE}
options(width = 90)
library(knitr)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1), las = 1)  # smaller margin on top and right
})
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(ggplot2)
```

## Important Maximimum Entropy Distributions

* If $\Theta$ is some convex set, the maximum entropy distribution is the uniform distribution
  over $\Theta$. For example, if $\Theta = \left[0,1\right]$, it is the standard uniform distribution
  with PDF $f\left(\left.\theta\right|a=0,b=1\right) = 1$
* If $\Theta = \mathbb{R}$, the maximum entropy distribution given an expectation and variance
  is the normal distribution. This extends to bivariate and multivariate distributions if you have
  given covariances.
* If $\Theta = \mathbb{R}_+$, then the maximum entropy distribution for a given expectation is the 
  exponential distribution with expectation $\mu = \frac{1}{\lambda}$. You can utilize the
  fact that the median is $F^{-1}\left(0.5\right) = \mu \ln 2$ to go from the median to $\mu$.
* The binomial and Poisson distributions are maximum entropy distributions given $\mu$ for
  their respective $\Omega$
* Additional examples (often with weird constraints) are given at the bottom of
  https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution

## Do This Once on Each Computer You Use

- R comes with a terrible default coding for ordered factors in regressions known
  as "Helmert" contrasts
- Execute this once to change them to "treatment" contrasts, which is the conventional
  coding in the social sciences with dummy variables relative to a baseline category
```{r, eval = FALSE}
cat('options(contrasts = c(unordered = "contr.treatment", ordered = "contr.treatment"))',
    file = "~/.Rprofile", sep = "\n", append = TRUE)
```
- Without this, you will get a weird rotation of the coefficients on dummy
  variables derived from ordered factors
- `"contr.sum"` is another reasonable (but rare) choice

## Prior Predictive Distribution for Roach Study {.build}

```{tikz, fig.cap = "Roach Model", fig.ext = 'png', echo = FALSE}
\usetikzlibrary{bayesnet}
\begin{tikzpicture}[node distance=2cm, auto,>=latex', thick, scale = 0.07]

  % Define nodes

  % Y
  \node[obs]          (y)   {roaches}; %

  % Xs
  \node[obs, left=7 of y] (y1) {lag\_roaches}; %
  \node[obs, above=0.25 of y1] (T)  {treatment}; %
  \node[obs, above=1.0 of y, xshift=-3cm] (s) {senior}; %
  \node[obs, above=1.0 of y, xshift=-1.5cm] (o) {offset}; %
  
  % conditional mean function
  \node[det, right=3 of y1] (n) {$\eta$} ; %
  \node[det, right=5 of y1] (m) {$\mu$} ; %

  % parameters
  \node[latent, above=3.4 of n]   (a) {$\alpha$} ; %
  \node[latent, above=3.0 of y1]  (b1) {$\beta_1$}  ; %
  \node[latent, right=0.5 of b1]  (b2) {$\beta_2$}  ; %
  \node[latent, right=0.5 of b2]  (b3) {$\beta_3$}  ; %

  \edge {a,b1,b2,b3,y1,T,s,o} {n} ; %
  \edge {n} {m} ; %
  \node[const, right=0.4 of n, yshift=-0.25cm] (exp) {$\exp$} ; %
  
  % Factors
  \factor[left=of y] {y-f} {below:$\mathcal{P}$} {m} {y} ; %
  \factor[above=of a] {a-f} {right:$\mathcal{N}$} {} {a}; %
  \factor[above=of b1] {b1-f} {left:$\mathcal{N}$} {} {b1} ; %
  \factor[above=of b2] {b2-f} {right:$\mathcal{N}$} {} {b2} ; %
  \factor[above=of b3] {b3-f} {left:$\mathcal{N}$} {} {b3} ; %

  % Hyperparameters
  \node[const, above=0.4 of a-f, xshift=-0.2cm] (m_a) {$m_a$} ; %
  \node[const, above=0.4 of a-f, xshift=+0.2cm] (s_a) {$s_a$} ; %
  \edge[-] {m_a,s_a} {a-f} ; %
  \node[const, above=0.4 of b1-f, xshift=-0.25cm] (m_b1) {$m_{b_1}$} ; %
  \node[const, above=0.4 of b1-f, xshift=+0.25cm] (s_b1) {$s_{b_1}$} ; %
  \edge[-] {m_b1,s_b1} {b1-f} ; %
  \node[const, above=0.4 of b2-f, xshift=-0.25cm] (m_b2) {$m_{b_2}$} ; %
  \node[const, above=0.4 of b2-f, xshift=+0.25cm] (s_b2) {$s_{b_2}$} ; %
  \edge[-] {m_b2,s_b2} {b2-f} ; %
  \node[const, above=0.4 of b3-f, xshift=-0.25cm] (m_b3) {$m_{b_3}$} ; %
  \node[const, above=0.4 of b3-f, xshift=+0.25cm] (s_b3) {$s_{b_3}$} ; %
  \edge[-] {m_b3,s_b3} {b3-f} ; %

  % Plates
  \plate {yx} { %
    (y)(y-f)(y-f-caption) %
    (y1)(y-f)(y-f-caption) %
    (T)(y-f)(y-f-caption) %
    (s)(y-f)(y-f-caption) %
  } {$\forall n \in 1, 2, \dots, N$} ;
\end{tikzpicture}
```

## Prior Predictive Distribution in Symbols

$$
\alpha \thicksim \mathcal{N}\left(m_\alpha, s_\alpha\right) \\
\beta_1 \thicksim \mathcal{N}\left(m_{\beta_1}, s_{\beta_1}\right) \\
\beta_2 \thicksim \mathcal{N}\left(m_{\beta_2}, s_{\beta_2}\right) \\
\beta_3 \thicksim \mathcal{N}\left(m_{\beta_3}, s_{\beta_3}\right) \\
\forall n: \eta_n = \alpha + OFFSET_n + 
  \beta_1 \times \log LAG_n + \beta_2 \times SENIOR_n + \beta_3 \times T_n \\
\forall n: \mu_n = e^{\eta_n} \\
\forall n: Y_n \thicksim \mathcal{Poisson}\left(\mu_n\right)
$$

## Breakout Rooms

Draw $S = 1000$ times (using `replicate`) from a prior predictive distribution
```{r}
roaches <- roaches[roaches$roach1 > 0, ]; str(roaches)
```
* The average number of pre-treatment roaches (`roaches$roach1`) in a building was about $42$ with
  a standard deviation of $75$
* Use `log(roach1)` as the predictor
* The offset is `log(roaches$exposure2)`
* Remember to center the predictors, including the offset, when predicting

## Posterior Distribution 

```{r, roaches, cache = TRUE, results = "hide"}
post <- stan_glm(y ~ senior + log(roach1) + treatment, data = roaches, 
                 family = poisson, offset = log(exposure2), QR = TRUE,
                 prior_intercept = normal(location = log(42), scale = 4, autoscale = FALSE),
                 prior = normal(location = 0, scale = c(5, 5, 1), autoscale = FALSE))
```
```{r, output.lines = -(1:6)}
print(post, digits = 2)
```

## Estimating Treatment Effects

```{r, fig.height = 3.75, fig.width = 10, small.mar = TRUE}
df <- roaches; df$treatment <- 0
Y_0 <- posterior_linpred(post, newdata = df, offset = log(df$exposure2), transform = TRUE)
df$treatment <- 1
Y_1 <- posterior_linpred(post, newdata = df, offset = log(df$exposure2), transform = TRUE)
plot(density(colMeans(Y_1 - Y_0), from = -75, to = 25), xlab = "Average Treatment Effect", main = "")
```

## Why NUTS Is Better than Other MCMC Samplers

* With Stan, it is almost always the case that things either go well or you get
  warning messages saying things did not go well
* Because Stan uses gradients, it scales well as models get more complex
* The first-order autocorrelation tends to be negative so you can get greater
  effective sample sizes (for mean estimates) than nominal sample size
```{r}
round(bayesplot::neff_ratio(post), digits = 2)
```

## Divergent Transitions

* NUTS only uses first derivatives
* First order approximations to Hamiltonian physiscs are fine for if either the second derivatives
  are constant or the discrete step size is sufficiently small
* When the second derviatives are very not constant across $\Theta$, Stan can (easily) mis-tune
  to a step size that is not sufficiently small and $\theta_k$ gets pushed to $\pm \infty$
* When this happens there will be a warning message, suggesting to increase `adapt_delta`
* When `adapt_delta` is closer to 1, Stan will tend to take smaller steps
* Unfortunately, even as `adapt_delta` $\lim 1$, there may be no sufficiently small
  step size and you need to try to reparameterize your model

## Exceeding Maximum Treedepth

* When the step size is small, NUTS needs many (small) steps to cross the "typical"
  subset of $\Theta$ and hit the U-turn point
* Sometimes, NUTS has not U-turned when it reaches its limit of 10 steps (by default)
* When this happens there will be a warning message, suggesting to increase `max_treedepth`
* There is always a sufficiently high value of `max_treedepth` to allow NUTS to
  reach the U-turn point, but increasing `max_treedepth` by 1 approximately doubles
  the wall time to obtain $S$ draws

## Low Bayesian Fraction of Missing Information

* When the tails of the posterior PDF are very light, NUTS can have difficulty moving
  through $\Theta$ efficiently
* This will manifest itself in a low (and possibly unreliable) estimate of $n_{eff}$  
* When this happens there will be a warning message, saying that the Bayesian Fraction
  of Missing Information (BFMI) is low
* In this situation, there is not much you can do except increase $S$ or preferably 
  reparameterize your model to make it easier for NUTS

## Runtime Exceptions

* Sometimes you will get a "informational" (not error, not warning) message saying
  that some parameter that should be positive is zero or some parameter that should
  be finite is infinite
* This means that a 64bit computer could not represent the number accurately
* If it only happens a few times and only during the warmup phase, do not worry
* Otherwise, you might try to use functions that are more numerically stable, which
  is discussed throughout the Stan User Manual 

## Initial Values

* Sometimes a Markov Chain will not get started because the log-kernel evaluates
  to $-\infty$ or `NaN` at the initial values (which are randomly generated)
* You can provide you own initial values
* But it is usually easier to specify `init_r` to be some value between 0 and 2
  (the default) which governs the range at which the initial values are drawn
  from on the unconstrained scale

## Tail / Bulk ESS

* Sometimes you will get a message about the tail or bulk ESS being low
* Call `monitor` on an object produced by Stan to see the estimates
* I do not worry too much about it if ONLY the `lp__` margin is too low
* Otherwise, you can increase the number of iterations and / or chains
* But it does suggest that there is something in the parameterization
  of your model that would make it difficult for Stan to sample from
  the implied posterior distribution

## ShinyStan

- ShinyStan can be launched on an object produced by rstanarm via
```{r, eval = FALSE, include = TRUE}
launch_shinystan(post)
```
- A webapp will open in your default web browser that helps you visualize
 the posterior distribution and diagnose problems

## Sensitivity to Individual Observations

```{r, fig.height=5, fig.width=10, warning = FALSE}
plot(loo(post), label_points = TRUE)
```

## Numerical Assessment of Calibration

```{r}
PPD <- posterior_predict(post); dim(PPD)
lower <- apply(PPD, MARGIN = 2, FUN = quantile, probs = 0.25)
upper <- apply(PPD, MARGIN = 2, FUN = quantile, probs = 0.75)
mean(roaches$y > lower & roaches$y < upper) # bad fit
```

* Overal, the model is fitting the data poorly
* You will often overfit when you lazily use all predictors that are available in
  the dataset

## Leave-One-Out Based Intervals

```{r, message = FALSE, warning = FALSE}
library(bayesplot)
pp_check(post, plotfun = "loo_intervals")
```

## Leave-One-Out Probability Integral Transform

```{r, warning = FALSE, message = FALSE}
pp_check(post, plotfun = "loo_pit_qq")
```

## Scatterplots 

```{r}
pp_check(post, plotfun = "scatter")
```

## Frequency Polygons

```{r, message = FALSE}
pp_check(post, plotfun = "freqpoly")
```

## Frequency Polygons with Grouping

```{r, message = FALSE}
pp_check(post, plotfun = "stat_freqpoly_grouped", group = roaches$treatment, stat = "IQR") + 
  legend_move("bottom")
```

## Empirical CDF

```{r}
pp_check(post, plotfun = "ecdf_overlay") + legend_move("bottom")
```

## MCMC Performance Plots
```{r}
format(available_mcmc(pattern = "data$", invert = TRUE))
```

## Autocorrelation Function

```{r}
mcmc_acf(post, lags = 5)
```

## Ridges

```{r}
mcmc_areas_ridges(post, regex_pars = "^[^(]") # exclude (Intercept)
```

## Hexograms

```{r}
mcmc_hex(post, pars = c("treatment", "senior"))
```

## Pairs

```{r, pairs, cache = TRUE}
pairs(post)
```

## Parallel Coordinates

```{r}
mcmc_parcoord(post, alpha = 0.1, np = nuts_params(post), 
              np_style = parcoord_style_np(div_color = "red", div_size = 1, div_alpha = 1))

```

## Prior Predictive Distribution for Well Switching {.build}

```{tikz, fig.cap = "Well Switching Model", fig.ext = 'png', echo = FALSE}
\usetikzlibrary{bayesnet}
\begin{tikzpicture}[node distance=2cm, auto,>=latex', thick, scale = 0.07]

  % Define nodes

  % Y
  \node[obs]          (y)   {switch?}; %

  % Xs
  \node[obs, left=7 of y] (d) {distance}; %
  \node[obs, above=0.25 of d] (t)  {arsenic}; %

  % conditional mean function
  \node[det, right=3 of d] (n) {$\eta$} ; %
  \node[det, right=5 of d] (m) {$\mu$} ; %

  % parameters
  \node[latent, above=3.4 of n]   (a) {$\alpha$} ; %
  \node[latent, above=3.0 of d]  (b1) {$\beta_1$}  ; %
  \node[latent, right=2 of b1]  (b2) {$\beta_2$}  ; %

  \edge {a,b1,b2,d,t} {n} ; %
  \edge {n} {m} ; %
  \node[const, right=0.4 of n, yshift=-0.5cm] (inv_logit) {logit$^{-1}$} ; %
  
  % Factors
  \factor[left=of y] {y-f} {below:$\mathcal{B}$} {m} {y} ; %
  \factor[above=of a] {a-f} {right:$\mathcal{N}$} {} {a}; %
  \factor[above=of b1] {b1-f} {left:$\mathcal{N}$} {} {b1} ; %
  \factor[above=of b2] {b2-f} {right:$\mathcal{N}$} {} {b2} ; %

  % Hyperparameters
  \node[const, above=0.4 of a-f, xshift=-0.2cm] (m_a) {$m_a$} ; %
  \node[const, above=0.4 of a-f, xshift=+0.2cm] (s_a) {$s_a$} ; %
  \edge[-] {m_a,s_a} {a-f} ; %
  \node[const, above=0.4 of b1-f, xshift=-0.25cm] (m_b1) {$m_{b_1}$} ; %
  \node[const, above=0.4 of b1-f, xshift=+0.25cm] (s_b1) {$s_{b_1}$} ; %
  \edge[-] {m_b1,s_b1} {b1-f} ; %
  \node[const, above=0.4 of b2-f, xshift=-0.25cm] (m_b2) {$m_{b_2}$} ; %
  \node[const, above=0.4 of b2-f, xshift=+0.25cm] (s_b2) {$s_{b_2}$} ; %
  \edge[-] {m_b2,s_b2} {b2-f} ; %

  % Plates
  \plate {yx} { %
    (y)(y-f)(y-f-caption) %
    (d)(y-f)(y-f-caption) %
    (t)(y-f)(y-f-caption) %
  } {$\forall n \in 1, 2, \dots, N$} ;
\end{tikzpicture}
```

## Prior Predictive Distribution in Symbols

$$
\alpha \thicksim \mathcal{N}\left(m_\alpha, s_\alpha\right) \\
\beta_1 \thicksim \mathcal{N}\left(m_{\beta_1}, s_{\beta_1}\right) \\
\beta_2 \thicksim \mathcal{N}\left(m_{\beta_2}, s_{\beta_2}\right) \\
\forall n: \eta_n = \alpha + \beta_1 \times ARSENIC_n + \beta_2 \times DISTANCE_n \\
\forall n: \epsilon_n \thicksim \mathcal{Logistic}\left(0,1\right) \\
\forall n: u_n = \eta_n + \epsilon_n \\
\forall n: Y_n = u_n > 0
$$

## Inverse Link Functions

```{r, echo = FALSE, small.mar = TRUE}
curve(plogis(eta), from = -5, to = 5, xname = "eta", xlab = "Linear Predictor",
      ylab = "Probability")
curve(pnorm(eta), from = -5, to = 5, xname = "eta", add = TRUE, col = 2, lty = 2)
legend("topleft", legend = c("Logistic", "Normal"), col = 1:2, lty = 1:2)
```


## Breakout Rooms

Draw $S = 1000$ times (using `replicate`) from a prior predictive distribution
```{r}
str(wells)
```
* You do not have to use `assoc` or `educ`
* To draw from the logistic distribution use `rlogis`
* Remember to center the predictors when predicting

## Posterior Distribution

```{r, logit, cache = TRUE, results = "hide"}
post <- stan_gamm4(switch ~ s(dist, arsenic), data = wells, family = binomial)
```
<div class="columns-2">
```{r, output.lines = -(1:6)}
print(post, digits = 2)
```
</div>

## Nonlinear Plot

```{r, message = FALSE, warning=FALSE, fig.height=5, fig.width=10}
plot_nonlinear(post)
```

## Plotting the Effect of an Increase in Arsenic

```{r, small.mar = TRUE, fig.height=4, fig.width=10}
mu_0 <- posterior_linpred(post, transform = TRUE)
df <- wells; df$arsenic <- df$arsenic + 1
mu_1 <- posterior_linpred(post, newdata = df, transform = TRUE)
plot(density(mu_1 - mu_0), main = "", xlab = "Change in Probabilty of Switching")
```

## Effective Number of Parameters

```{r}
loo(post) # nominal number of estimated parameters is 32
```

## A Binomial Model for Romney vs Obama in $2012$ {.smaller}

```{r, message = FALSE, warning = FALSE}
poll <- readRDS("GooglePoll.rds") # WantToWin is coded as 1 for Romney and 0 for Obama
library(dplyr)
collapsed <- filter(poll, !is.na(WantToWin)) %>%
             group_by(Region, Gender, Urban_Density, Age, Income) %>%
             summarize(Romney = sum(grepl("Romney", WantToWin)), Obama = n() - Romney) %>%
             na.omit
```
```{r, president, cache = TRUE, results = "hide"}
post <- stan_glm(cbind(Romney, Obama) ~ ., data = collapsed, family = binomial(link = "probit"), 
                 QR = TRUE, init_r = 0.25)
```
<div class="columns-2">
```{r, output.lines = 7:24}
print(post, digits = 2)
```
</div>
